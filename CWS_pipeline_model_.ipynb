{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1D0JDo3UveY_ZwdkHDdW3yhfhSFm-tZbt",
      "authorship_tag": "ABX9TyPA8mE+ubOOt4slnEgbQqDh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sandeep16064/MyCoref/blob/master/CWS_pipeline_model_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!bash /content/neuralcoref/neuralcoref/train/conll_processing_script/compile_coref_data.sh"
      ],
      "metadata": {
        "id": "6_4r7tQcRDeY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q condacolab"
      ],
      "metadata": {
        "id": "yik2-TQwOZL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import condacolab\n",
        "condacolab.install()"
      ],
      "metadata": {
        "id": "0GEOcti8JJf8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97d9f09c-2878-4379-ca2f-bfa9126fe948"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⏬ Downloading https://github.com/jaimergp/miniforge/releases/latest/download/Mambaforge-colab-Linux-x86_64.sh...\n",
            "📦 Installing...\n",
            "📌 Adjusting configuration...\n",
            "🩹 Patching environment...\n",
            "⏲ Done in 0:00:30\n",
            "🔁 Restarting kernel...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!conda install pytorch -c pytorch\n"
      ],
      "metadata": {
        "id": "z9T2ds-tHK-S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31572d00-e00a-4ce8-819b-7571440bd099"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting package metadata (current_repodata.json): - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\bdone\n",
            "Solving environment: - \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "\n",
            "## Package Plan ##\n",
            "\n",
            "  environment location: /usr/local\n",
            "\n",
            "  added / updated specs:\n",
            "    - pytorch\n",
            "\n",
            "\n",
            "The following packages will be downloaded:\n",
            "\n",
            "    package                    |            build\n",
            "    ---------------------------|-----------------\n",
            "    _openmp_mutex-4.5          |       2_kmp_llvm           6 KB  conda-forge\n",
            "    blas-2.106                 |              mkl          12 KB  conda-forge\n",
            "    ca-certificates-2021.10.8  |       ha878542_0         139 KB  conda-forge\n",
            "    certifi-2021.10.8          |   py37h89c1867_2         145 KB  conda-forge\n",
            "    conda-4.12.0               |   py37h89c1867_0         1.0 MB  conda-forge\n",
            "    cpuonly-1.0                |                0           2 KB  pytorch\n",
            "    cudatoolkit-11.1.1         |       h6406543_8        1.20 GB  conda-forge\n",
            "    libblas-3.9.0              |            6_mkl          11 KB  conda-forge\n",
            "    libcblas-3.9.0             |            6_mkl          11 KB  conda-forge\n",
            "    libgfortran-ng-11.2.0      |      h69a702a_16          23 KB  conda-forge\n",
            "    libgfortran5-11.2.0        |      h5c6108e_16         1.7 MB  conda-forge\n",
            "    liblapack-3.9.0            |            6_mkl          11 KB  conda-forge\n",
            "    liblapacke-3.9.0           |            6_mkl          11 KB  conda-forge\n",
            "    libzlib-1.2.11             |    h36c2ea0_1013          59 KB  conda-forge\n",
            "    llvm-openmp-14.0.3         |       he0ac6c6_0         5.8 MB  conda-forge\n",
            "    mkl-2020.4                 |     h726a3e6_304       215.6 MB  conda-forge\n",
            "    ninja-1.10.2               |       h4bd325d_0         2.4 MB  conda-forge\n",
            "    numpy-1.20.3               |   py37h038b26d_1         5.7 MB  conda-forge\n",
            "    python_abi-3.7             |          2_cp37m           4 KB  conda-forge\n",
            "    pytorch-1.6.0              |      py3.7_cpu_0        59.4 MB  pytorch\n",
            "    zlib-1.2.11                |    h36c2ea0_1013          86 KB  conda-forge\n",
            "    ------------------------------------------------------------\n",
            "                                           Total:        1.49 GB\n",
            "\n",
            "The following NEW packages will be INSTALLED:\n",
            "\n",
            "  blas               conda-forge/linux-64::blas-2.106-mkl\n",
            "  cpuonly            pytorch/noarch::cpuonly-1.0-0\n",
            "  cudatoolkit        conda-forge/linux-64::cudatoolkit-11.1.1-h6406543_8\n",
            "  libblas            conda-forge/linux-64::libblas-3.9.0-6_mkl\n",
            "  libcblas           conda-forge/linux-64::libcblas-3.9.0-6_mkl\n",
            "  libgfortran-ng     conda-forge/linux-64::libgfortran-ng-11.2.0-h69a702a_16\n",
            "  libgfortran5       conda-forge/linux-64::libgfortran5-11.2.0-h5c6108e_16\n",
            "  liblapack          conda-forge/linux-64::liblapack-3.9.0-6_mkl\n",
            "  liblapacke         conda-forge/linux-64::liblapacke-3.9.0-6_mkl\n",
            "  libzlib            conda-forge/linux-64::libzlib-1.2.11-h36c2ea0_1013\n",
            "  llvm-openmp        conda-forge/linux-64::llvm-openmp-14.0.3-he0ac6c6_0\n",
            "  mkl                conda-forge/linux-64::mkl-2020.4-h726a3e6_304\n",
            "  ninja              conda-forge/linux-64::ninja-1.10.2-h4bd325d_0\n",
            "  numpy              conda-forge/linux-64::numpy-1.20.3-py37h038b26d_1\n",
            "  pytorch            pytorch/linux-64::pytorch-1.6.0-py3.7_cpu_0\n",
            "\n",
            "The following packages will be UPDATED:\n",
            "\n",
            "  ca-certificates                      2020.12.5-ha878542_0 --> 2021.10.8-ha878542_0\n",
            "  certifi                          2020.12.5-py37h89c1867_1 --> 2021.10.8-py37h89c1867_2\n",
            "  conda                                4.9.2-py37h89c1867_0 --> 4.12.0-py37h89c1867_0\n",
            "  python_abi                                    3.7-1_cp37m --> 3.7-2_cp37m\n",
            "  zlib                                 1.2.11-h516909a_1010 --> 1.2.11-h36c2ea0_1013\n",
            "\n",
            "The following packages will be DOWNGRADED:\n",
            "\n",
            "  _openmp_mutex                                   4.5-1_gnu --> 4.5-2_kmp_llvm\n",
            "\n",
            "\n",
            "\n",
            "Downloading and Extracting Packages\n",
            "certifi-2021.10.8    | 145 KB    | : 100% 1.0/1 [00:00<00:00,  7.86it/s]\n",
            "python_abi-3.7       | 4 KB      | : 100% 1.0/1 [00:00<00:00, 33.05it/s]\n",
            "liblapack-3.9.0      | 11 KB     | : 100% 1.0/1 [00:00<00:00, 20.18it/s]\n",
            "libgfortran-ng-11.2. | 23 KB     | : 100% 1.0/1 [00:00<00:00, 23.28it/s]\n",
            "mkl-2020.4           | 215.6 MB  | : 100% 1.0/1 [00:40<00:00, 40.18s/it] \n",
            "liblapacke-3.9.0     | 11 KB     | : 100% 1.0/1 [00:00<00:00,  7.71it/s]\n",
            "numpy-1.20.3         | 5.7 MB    | : 100% 1.0/1 [00:01<00:00,  1.20s/it]\n",
            "ninja-1.10.2         | 2.4 MB    | : 100% 1.0/1 [00:00<00:00,  2.05it/s]\n",
            "ca-certificates-2021 | 139 KB    | : 100% 1.0/1 [00:00<00:00, 12.80it/s]\n",
            "libblas-3.9.0        | 11 KB     | : 100% 1.0/1 [00:00<00:00,  8.04it/s]\n",
            "cudatoolkit-11.1.1   | 1.20 GB   | : 100% 1.0/1 [03:01<00:00, 181.85s/it]              \n",
            "conda-4.12.0         | 1.0 MB    | : 100% 1.0/1 [00:00<00:00,  3.53it/s]\n",
            "libzlib-1.2.11       | 59 KB     | : 100% 1.0/1 [00:00<00:00, 23.72it/s]\n",
            "libcblas-3.9.0       | 11 KB     | : 100% 1.0/1 [00:00<00:00, 27.14it/s]\n",
            "pytorch-1.6.0        | 59.4 MB   | : 100% 1.0/1 [00:14<00:00, 14.35s/it]\n",
            "blas-2.106           | 12 KB     | : 100% 1.0/1 [00:00<00:00, 24.36it/s]\n",
            "libgfortran5-11.2.0  | 1.7 MB    | : 100% 1.0/1 [00:00<00:00,  2.95it/s]\n",
            "zlib-1.2.11          | 86 KB     | : 100% 1.0/1 [00:00<00:00, 18.40it/s]\n",
            "_openmp_mutex-4.5    | 6 KB      | : 100% 1.0/1 [00:00<00:00, 21.15it/s]\n",
            "cpuonly-1.0          | 2 KB      | : 100% 1.0/1 [00:00<00:00,  1.88it/s]\n",
            "llvm-openmp-14.0.3   | 5.8 MB    | : 100% 1.0/1 [00:00<00:00,  1.09it/s]               \n",
            "Preparing transaction: | \b\b/ \b\b- \b\bdone\n",
            "Verifying transaction: | \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\bdone\n",
            "Executing transaction: | \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ \b\b- \b\b\\ \b\b| \b\b/ By downloading and using the CUDA Toolkit conda packages, you accept the terms and conditions of the CUDA End User License Agreement (EULA): https://docs.nvidia.com/cuda/eula/index.html\n",
            "\n",
            "\b\b- \b\b\\ \b\bdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#%cd ../"
      ],
      "metadata": {
        "id": "Lc3wqT_tRqs9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "nMnN2d7dahe1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/huggingface/neuralcoref.git"
      ],
      "metadata": {
        "id": "RqyALuxJldkQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23b889fd-1b59-43eb-c3d8-ccb02e41b43d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'neuralcoref'...\n",
            "remote: Enumerating objects: 772, done.\u001b[K\n",
            "remote: Counting objects: 100% (24/24), done.\u001b[K\n",
            "remote: Compressing objects: 100% (16/16), done.\u001b[K\n",
            "remote: Total 772 (delta 10), reused 16 (delta 7), pack-reused 748\u001b[K\n",
            "Receiving objects: 100% (772/772), 67.85 MiB | 20.62 MiB/s, done.\n",
            "Resolving deltas: 100% (407/407), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd neuralcoref"
      ],
      "metadata": {
        "id": "swCdHVJWpLKC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "409b69ef-8ad2-4205-a5fb-6e4afa1e573c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/neuralcoref\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Pxxxe6W9aki8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aaae1944-9f70-4ba3-ba95-6ba505ce5f71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/bin/python: No module named spacy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#%cd /content/neuralcoref/neuralcoref/train"
      ],
      "metadata": {
        "id": "U_PSGRBbSPpq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r ./neuralcoref/train/training_requirements.txt"
      ],
      "metadata": {
        "id": "r9v6FLz2SiGX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd43901f-370a-4075-8b48-64a156b21aa6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting spacy\n",
            "  Downloading spacy-3.3.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.2 MB 11.8 MB/s \n",
            "\u001b[?25hCollecting torch<1.4.0,>=1.3.0\n",
            "  Downloading torch-1.3.1-cp37-cp37m-manylinux1_x86_64.whl (734.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 734.6 MB 19 kB/s \n",
            "\u001b[?25hCollecting tensorboardX\n",
            "  Downloading tensorboardX-2.5-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 52.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/site-packages (from torch<1.4.0,>=1.3.0->-r ./neuralcoref/train/training_requirements.txt (line 2)) (1.20.3)\n",
            "Collecting catalogue<2.1.0,>=2.0.6\n",
            "  Downloading catalogue-2.0.7-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/site-packages (from spacy->-r ./neuralcoref/train/training_requirements.txt (line 1)) (2.25.1)\n",
            "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
            "  Downloading pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.1 MB 36.3 MB/s \n",
            "\u001b[?25hCollecting spacy-legacy<3.1.0,>=3.0.9\n",
            "  Downloading spacy_legacy-3.0.9-py2.py3-none-any.whl (20 kB)\n",
            "Collecting murmurhash<1.1.0,>=0.28.0\n",
            "  Downloading murmurhash-1.0.7-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21 kB)\n",
            "Collecting typing-extensions<4.0.0.0,>=3.7.4\n",
            "  Downloading typing_extensions-3.10.0.2-py3-none-any.whl (26 kB)\n",
            "Collecting wasabi<1.1.0,>=0.9.1\n",
            "  Downloading wasabi-0.9.1-py3-none-any.whl (26 kB)\n",
            "Collecting thinc<8.1.0,>=8.0.14\n",
            "  Downloading thinc-8.0.15-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (653 kB)\n",
            "\u001b[K     |████████████████████████████████| 653 kB 41.8 MB/s \n",
            "\u001b[?25hCollecting srsly<3.0.0,>=2.4.3\n",
            "  Downloading srsly-2.4.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (457 kB)\n",
            "\u001b[K     |████████████████████████████████| 457 kB 49.8 MB/s \n",
            "\u001b[?25hCollecting cymem<2.1.0,>=2.0.2\n",
            "  Downloading cymem-2.0.6-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35 kB)\n",
            "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
            "  Downloading spacy_loggers-1.0.2-py3-none-any.whl (7.2 kB)\n",
            "Collecting blis<0.8.0,>=0.4.0\n",
            "  Downloading blis-0.7.7-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.9 MB 41.3 MB/s \n",
            "\u001b[?25hCollecting jinja2\n",
            "  Downloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
            "\u001b[K     |████████████████████████████████| 133 kB 53.4 MB/s \n",
            "\u001b[?25hCollecting preshed<3.1.0,>=3.0.2\n",
            "  Downloading preshed-3.0.6-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 49.4 MB/s \n",
            "\u001b[?25hCollecting langcodes<4.0.0,>=3.2.0\n",
            "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 49.9 MB/s \n",
            "\u001b[?25hCollecting pathy>=0.3.5\n",
            "  Downloading pathy-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[K     |████████████████████████████████| 42 kB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/site-packages (from spacy->-r ./neuralcoref/train/training_requirements.txt (line 1)) (4.59.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/site-packages (from spacy->-r ./neuralcoref/train/training_requirements.txt (line 1)) (49.6.0.post20210108)\n",
            "Collecting typer<0.5.0,>=0.3.0\n",
            "  Downloading typer-0.4.1-py3-none-any.whl (27 kB)\n",
            "Collecting packaging>=20.0\n",
            "  Downloading packaging-21.3-py3-none-any.whl (40 kB)\n",
            "\u001b[K     |████████████████████████████████| 40 kB 5.3 MB/s \n",
            "\u001b[?25hCollecting zipp>=0.5\n",
            "  Downloading zipp-3.8.0-py3-none-any.whl (5.4 kB)\n",
            "Collecting pyparsing!=3.0.5,>=2.0.2\n",
            "  Downloading pyparsing-3.0.8-py3-none-any.whl (98 kB)\n",
            "\u001b[K     |████████████████████████████████| 98 kB 3.1 MB/s \n",
            "\u001b[?25hCollecting smart-open<6.0.0,>=5.0.0\n",
            "  Downloading smart_open-5.2.1-py3-none-any.whl (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 5.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy->-r ./neuralcoref/train/training_requirements.txt (line 1)) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy->-r ./neuralcoref/train/training_requirements.txt (line 1)) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy->-r ./neuralcoref/train/training_requirements.txt (line 1)) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy->-r ./neuralcoref/train/training_requirements.txt (line 1)) (1.26.3)\n",
            "Collecting click<9.0.0,>=7.1.1\n",
            "  Downloading click-8.1.3-py3-none-any.whl (96 kB)\n",
            "\u001b[K     |████████████████████████████████| 96 kB 6.0 MB/s \n",
            "\u001b[?25hCollecting importlib-metadata\n",
            "  Downloading importlib_metadata-4.11.3-py3-none-any.whl (18 kB)\n",
            "Collecting protobuf>=3.8.0\n",
            "  Downloading protobuf-3.20.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 41.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/site-packages (from tensorboardX->-r ./neuralcoref/train/training_requirements.txt (line 3)) (1.15.0)\n",
            "Collecting MarkupSafe>=2.0\n",
            "  Downloading MarkupSafe-2.1.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Installing collected packages: zipp, typing-extensions, importlib-metadata, murmurhash, cymem, click, catalogue, wasabi, typer, srsly, smart-open, pyparsing, pydantic, preshed, MarkupSafe, blis, thinc, spacy-loggers, spacy-legacy, protobuf, pathy, packaging, langcodes, jinja2, torch, tensorboardX, spacy\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.6.0\n",
            "    Uninstalling torch-1.6.0:\n",
            "      Successfully uninstalled torch-1.6.0\n",
            "Successfully installed MarkupSafe-2.1.1 blis-0.7.7 catalogue-2.0.7 click-8.1.3 cymem-2.0.6 importlib-metadata-4.11.3 jinja2-3.1.2 langcodes-3.3.0 murmurhash-1.0.7 packaging-21.3 pathy-0.6.1 preshed-3.0.6 protobuf-3.20.1 pydantic-1.8.2 pyparsing-3.0.8 smart-open-5.2.1 spacy-3.3.0 spacy-legacy-3.0.9 spacy-loggers-1.0.2 srsly-2.4.3 tensorboardX-2.5 thinc-8.0.15 torch-1.3.1 typer-0.4.1 typing-extensions-3.10.0.2 wasabi-0.9.1 zipp-3.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -e ."
      ],
      "metadata": {
        "id": "XGMQV7MvbX9E",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d1c090a3-b309-41b7-bfa9-540eb7951192"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obtaining file:///content/neuralcoref\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/site-packages (from neuralcoref==4.0) (1.20.3)\n",
            "Collecting boto3\n",
            "  Downloading boto3-1.22.4-py3-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 12.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/site-packages (from neuralcoref==4.0) (2.25.1)\n",
            "Collecting spacy<3.0.0,>=2.1.0\n",
            "  Using cached spacy-2.3.7-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.4 MB)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->neuralcoref==4.0) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->neuralcoref==4.0) (1.26.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->neuralcoref==4.0) (2021.10.8)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->neuralcoref==4.0) (4.0.0)\n",
            "Collecting srsly<1.1.0,>=1.0.2\n",
            "  Using cached srsly-1.0.5-cp37-cp37m-manylinux2014_x86_64.whl (184 kB)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/site-packages (from spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (2.0.6)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/site-packages (from spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (0.7.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/site-packages (from spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (3.0.6)\n",
            "Collecting plac<1.2.0,>=0.9.6\n",
            "  Using cached plac-1.1.3-py2.py3-none-any.whl (20 kB)\n",
            "Collecting thinc<7.5.0,>=7.4.1\n",
            "  Using cached thinc-7.4.5-cp37-cp37m-manylinux2014_x86_64.whl (1.0 MB)\n",
            "Collecting catalogue<1.1.0,>=0.0.7\n",
            "  Using cached catalogue-1.0.0-py2.py3-none-any.whl (7.7 kB)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/site-packages (from spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (0.9.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/site-packages (from spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (4.59.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/site-packages (from spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (49.6.0.post20210108)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/site-packages (from spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (1.0.7)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/site-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/site-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (3.10.0.2)\n",
            "Collecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.0-py3-none-any.whl (23 kB)\n",
            "Collecting s3transfer<0.6.0,>=0.5.0\n",
            "  Downloading s3transfer-0.5.2-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 8.7 MB/s \n",
            "\u001b[?25hCollecting botocore<1.26.0,>=1.25.4\n",
            "  Downloading botocore-1.25.4-py3-none-any.whl (8.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.7 MB 57.3 MB/s \n",
            "\u001b[?25hCollecting python-dateutil<3.0.0,>=2.1\n",
            "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
            "\u001b[K     |████████████████████████████████| 247 kB 52.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.26.0,>=1.25.4->boto3->neuralcoref==4.0) (1.15.0)\n",
            "Installing collected packages: python-dateutil, jmespath, srsly, plac, catalogue, botocore, thinc, s3transfer, spacy, boto3, neuralcoref\n",
            "  Attempting uninstall: srsly\n",
            "    Found existing installation: srsly 2.4.3\n",
            "    Uninstalling srsly-2.4.3:\n",
            "      Successfully uninstalled srsly-2.4.3\n",
            "  Attempting uninstall: catalogue\n",
            "    Found existing installation: catalogue 2.0.7\n",
            "    Uninstalling catalogue-2.0.7:\n",
            "      Successfully uninstalled catalogue-2.0.7\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.0.15\n",
            "    Uninstalling thinc-8.0.15:\n",
            "      Successfully uninstalled thinc-8.0.15\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.3.0\n",
            "    Uninstalling spacy-3.3.0:\n",
            "      Successfully uninstalled spacy-3.3.0\n",
            "  Running setup.py develop for neuralcoref\n",
            "Successfully installed boto3-1.22.4 botocore-1.25.4 catalogue-1.0.0 jmespath-1.0.0 neuralcoref plac-1.1.3 python-dateutil-2.8.2 s3transfer-0.5.2 spacy-2.3.7 srsly-1.0.5 thinc-7.4.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "dateutil"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#or en_core_web_sm\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8SJZz0_cSmqx",
        "outputId": "f63447b0-108c-4cd4-f3bd-ac582da123a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en_core_web_sm==2.3.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz (12.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.0 MB 12.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<2.4.0,>=2.3.0 in /usr/local/lib/python3.7/site-packages (from en_core_web_sm==2.3.1) (2.3.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.6)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.7.7)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.1.3)\n",
            "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in /usr/local/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (7.4.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.9.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.7)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.20.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.25.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.59.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (49.6.0.post20210108)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.6)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/site-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/site-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/site-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.10.0.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.26.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.10)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.7/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.0.0)\n",
            "Building wheels for collected packages: en-core-web-sm\n",
            "  Building wheel for en-core-web-sm (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-sm: filename=en_core_web_sm-2.3.1-py3-none-any.whl size=12047106 sha256=8b62683eda2d8e177bddac3125c77d6b96d7a2c30058a23f52842eccd250c91c\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/0d/f0/7ecae8427c515065d75410989e15e5785dd3975fe06e795cd9\n",
            "Successfully built en-core-web-sm\n",
            "Installing collected packages: en-core-web-sm\n",
            "Successfully installed en-core-web-sm-2.3.1\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!python -m spacy download en"
      ],
      "metadata": {
        "id": "jvF03-ZEpBWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m neuralcoref.train.conllparser --path /content/drive/MyDrive/data/train/"
      ],
      "metadata": {
        "id": "jyFfEmHg4oWC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7601c30b-3197-4a76-fa18-2573547e0bb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: spacy.vocab.Vocab size changed, may indicate binary incompatibility. Expected 104 from C header, got 112 from PyObject\n",
            "  return f(*args, **kwds)\n",
            "100% 40155833/40155833 [00:02<00:00, 19563079.59B/s]\n",
            "Loading embeddings from /content/neuralcoref/neuralcoref/train/weights/static_word\n",
            "Loading embeddings from /content/neuralcoref/neuralcoref/train/weights/tuned_word\n",
            "🌋 Reading files\n",
            "In /content/drive/MyDrive/data/train/ /content/drive/MyDrive/data/train\n",
            "In /content/drive/MyDrive/data/train/.ipynb_checkpoints /content/drive/MyDrive/data/train/.ipynb_checkpoints\n",
            "In /content/drive/MyDrive/data/train/numpy /content/drive/MyDrive/data/train/numpy\n",
            "utts_text size 75187\n",
            "utts_tokens size 75187\n",
            "utts_corefs size 75187\n",
            "utts_speakers size 75187\n",
            "utts_doc_idx size 75187\n",
            "🌋 Building docs\n",
            "🌋 Loading spacy model\n",
            "\n",
            "\u001b[38;5;1m✘ Can't find model meta.json\u001b[0m\n",
            "/usr/local/lib/python3.7/site-packages/spacy/data/en_core_web_lg/meta.json\n",
            "\n",
            "Could not detect model en_core_web_lg\n",
            "\n",
            "\u001b[38;5;1m✘ Can't find model meta.json\u001b[0m\n",
            "/usr/local/lib/python3.7/site-packages/spacy/data/en_core_web_md/meta.json\n",
            "\n",
            "Could not detect model en_core_web_md\n",
            "\u001b[1m\n",
            "===================== Info about model 'en_core_web_sm' =====================\u001b[0m\n",
            "\n",
            "lang             en                            \n",
            "name             core_web_sm                   \n",
            "license          MIT                           \n",
            "author           Explosion                     \n",
            "url              https://explosion.ai          \n",
            "email            contact@explosion.ai          \n",
            "description      English multi-task CNN trained on OntoNotes. Assigns context-specific token vectors, POS tags, dependency parse and named entities.\n",
            "sources          [{'name': 'OntoNotes 5', 'url': 'https://catalog.ldc.upenn.edu/LDC2013T19', 'license': 'commercial (licensed by Explosion)'}]\n",
            "pipeline         ['tagger', 'parser', 'ner']   \n",
            "version          2.3.1                         \n",
            "spacy_version    >=2.3.0,<2.4.0                \n",
            "parent_package   spacy                         \n",
            "labels           {'tagger': ['$', \"''\", ',', '-LRB-', '-RRB-', '.', ':', 'ADD', 'AFX', 'CC', 'CD', 'DT', 'EX', 'FW', 'HYPH', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NFP', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', 'XX', '_SP', '``'], 'parser': ['ROOT', 'acl', 'acomp', 'advcl', 'advmod', 'agent', 'amod', 'appos', 'attr', 'aux', 'auxpass', 'case', 'cc', 'ccomp', 'compound', 'conj', 'csubj', 'csubjpass', 'dative', 'dep', 'det', 'dobj', 'expl', 'intj', 'mark', 'meta', 'neg', 'nmod', 'npadvmod', 'nsubj', 'nsubjpass', 'nummod', 'oprd', 'parataxis', 'pcomp', 'pobj', 'poss', 'preconj', 'predet', 'prep', 'prt', 'punct', 'quantmod', 'relcl', 'xcomp'], 'ner': ['CARDINAL', 'DATE', 'EVENT', 'FAC', 'GPE', 'LANGUAGE', 'LAW', 'LOC', 'MONEY', 'NORP', 'ORDINAL', 'ORG', 'PERCENT', 'PERSON', 'PRODUCT', 'QUANTITY', 'TIME', 'WORK_OF_ART']}\n",
            "source           /usr/local/lib/python3.7/site-packages/en_core_web_sm\n",
            "\n",
            "Loading model en_core_web_sm\n",
            "🌋 Parsing utterances and filling docs with use_gold_mentions=False\n",
            "75187it [03:57, 315.98it/s]\n",
            "=> read_corpus time elapsed 246.6607072353363\n",
            "🌋 Extracting mentions features with 1 job(s)\n",
            "100% 2792/2792 [04:47<00:00,  9.70it/s]\n",
            "🌋 Building and gathering array with 1 job(s)\n",
            " 58% 1615/2792 [13:43<28:40,  1.46s/it]Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
            "    \"__main__\", mod_spec)\n",
            "  File \"/usr/local/lib/python3.7/runpy.py\", line 85, in _run_code\n",
            "    exec(code, run_globals)\n",
            "  File \"/content/neuralcoref/neuralcoref/train/conllparser.py\", line 946, in <module>\n",
            "    CORPUS.build_and_gather_multiple_arrays(SAVE_DIR)\n",
            "  File \"/content/neuralcoref/neuralcoref/train/conllparser.py\", line 809, in build_and_gather_multiple_arrays\n",
            "    arr, get_feats, use_kwargs=True, n_jobs=self.n_jobs\n",
            "  File \"/content/neuralcoref/neuralcoref/train/utils.py\", line 91, in parallel_process\n",
            "    for a in tqdm(array[front_num:])\n",
            "  File \"/content/neuralcoref/neuralcoref/train/utils.py\", line 91, in <listcomp>\n",
            "    for a in tqdm(array[front_num:])\n",
            "  File \"/content/neuralcoref/neuralcoref/train/conllparser.py\", line 282, in get_feats\n",
            "    return doc.get_feature_array(doc_id=i)\n",
            "  File \"/content/neuralcoref/neuralcoref/train/conllparser.py\", line 521, in get_feature_array\n",
            "    for ant in ants\n",
            "  File \"/content/neuralcoref/neuralcoref/train/conllparser.py\", line 521, in <listcomp>\n",
            "    for ant in ants\n",
            "  File \"/content/neuralcoref/neuralcoref/train/conllparser.py\", line 443, in get_pair_mentions_features_conll\n",
            "    features_, _ = self.get_pair_mentions_features(m1, m2)\n",
            "  File \"/content/neuralcoref/neuralcoref/train/document.py\", line 895, in get_pair_mentions_features\n",
            "    return (features_, np.concatenate(pairwise_features, axis=0))\n",
            "  File \"<__array_function__ internals>\", line 6, in concatenate\n",
            "KeyboardInterrupt\n",
            " 58% 1615/2792 [16:23<11:57,  1.64it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m neuralcoref.train.conllparser --path /content/drive/MyDrive/data/dev/"
      ],
      "metadata": {
        "id": "w8UuUGAci9qz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bce18c6-dc79-4c6d-e43c-dfa647be7363"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: spacy.vocab.Vocab size changed, may indicate binary incompatibility. Expected 104 from C header, got 112 from PyObject\n",
            "  return f(*args, **kwds)\n",
            "Loading embeddings from /content/neuralcoref/neuralcoref/train/weights/static_word\n",
            "Loading embeddings from /content/neuralcoref/neuralcoref/train/weights/tuned_word\n",
            "🌋 Reading files\n",
            "In /content/drive/MyDrive/data/dev/ /content/drive/MyDrive/data/dev\n",
            "In /content/drive/MyDrive/data/dev/numpy /content/drive/MyDrive/data/dev/numpy\n",
            "utts_text size 9603\n",
            "utts_tokens size 9603\n",
            "utts_corefs size 9603\n",
            "utts_speakers size 9603\n",
            "utts_doc_idx size 9603\n",
            "🌋 Building docs\n",
            "🌋 Loading spacy model\n",
            "\n",
            "\u001b[38;5;1m✘ Can't find model meta.json\u001b[0m\n",
            "/usr/local/lib/python3.7/site-packages/spacy/data/en_core_web_lg/meta.json\n",
            "\n",
            "Could not detect model en_core_web_lg\n",
            "\n",
            "\u001b[38;5;1m✘ Can't find model meta.json\u001b[0m\n",
            "/usr/local/lib/python3.7/site-packages/spacy/data/en_core_web_md/meta.json\n",
            "\n",
            "Could not detect model en_core_web_md\n",
            "\u001b[1m\n",
            "===================== Info about model 'en_core_web_sm' =====================\u001b[0m\n",
            "\n",
            "lang             en                            \n",
            "name             core_web_sm                   \n",
            "license          MIT                           \n",
            "author           Explosion                     \n",
            "url              https://explosion.ai          \n",
            "email            contact@explosion.ai          \n",
            "description      English multi-task CNN trained on OntoNotes. Assigns context-specific token vectors, POS tags, dependency parse and named entities.\n",
            "sources          [{'name': 'OntoNotes 5', 'url': 'https://catalog.ldc.upenn.edu/LDC2013T19', 'license': 'commercial (licensed by Explosion)'}]\n",
            "pipeline         ['tagger', 'parser', 'ner']   \n",
            "version          2.3.1                         \n",
            "spacy_version    >=2.3.0,<2.4.0                \n",
            "parent_package   spacy                         \n",
            "labels           {'tagger': ['$', \"''\", ',', '-LRB-', '-RRB-', '.', ':', 'ADD', 'AFX', 'CC', 'CD', 'DT', 'EX', 'FW', 'HYPH', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NFP', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', 'XX', '_SP', '``'], 'parser': ['ROOT', 'acl', 'acomp', 'advcl', 'advmod', 'agent', 'amod', 'appos', 'attr', 'aux', 'auxpass', 'case', 'cc', 'ccomp', 'compound', 'conj', 'csubj', 'csubjpass', 'dative', 'dep', 'det', 'dobj', 'expl', 'intj', 'mark', 'meta', 'neg', 'nmod', 'npadvmod', 'nsubj', 'nsubjpass', 'nummod', 'oprd', 'parataxis', 'pcomp', 'pobj', 'poss', 'preconj', 'predet', 'prep', 'prt', 'punct', 'quantmod', 'relcl', 'xcomp'], 'ner': ['CARDINAL', 'DATE', 'EVENT', 'FAC', 'GPE', 'LANGUAGE', 'LAW', 'LOC', 'MONEY', 'NORP', 'ORDINAL', 'ORG', 'PERCENT', 'PERSON', 'PRODUCT', 'QUANTITY', 'TIME', 'WORK_OF_ART']}\n",
            "source           /usr/local/lib/python3.7/site-packages/en_core_web_sm\n",
            "\n",
            "Loading model en_core_web_sm\n",
            "🌋 Parsing utterances and filling docs with use_gold_mentions=False\n",
            "9603it [00:30, 313.87it/s]\n",
            "=> read_corpus time elapsed 33.16603994369507\n",
            "🌋 Extracting mentions features with 1 job(s)\n",
            "100% 333/333 [00:35<00:00,  9.35it/s]\n",
            "🌋 Building and gathering array with 1 job(s)\n",
            "100% 333/333 [04:45<00:00,  1.17it/s]\n",
            "100% 343/343 [00:01<00:00, 280.58it/s]\n",
            "Building numpy array for mentions_features length 59866\n",
            "Saving numpy mentions_features size (59866, 6)\n",
            "Building numpy array for mentions_labels length 59866\n",
            "Saving numpy mentions_labels size (59866, 1)\n",
            "Building numpy array for mentions_pairs_length length 59866\n",
            "Saving numpy mentions_pairs_length size (59866, 1)\n",
            "Building numpy array for mentions_pairs_start_index length 59866\n",
            "Saving numpy mentions_pairs_start_index size (59866, 1)\n",
            "Building numpy array for mentions_spans length 59866\n",
            "Saving numpy mentions_spans size (59866, 250)\n",
            "Building numpy array for mentions_words length 59866\n",
            "Saving numpy mentions_words size (59866, 8)\n",
            "Building numpy array for pairs_ant_index length 8308185\n",
            "Saving numpy pairs_ant_index size (8308185, 1)\n",
            "Building numpy array for pairs_features length 8308185\n",
            "Saving numpy pairs_features size (8308185, 9)\n",
            "Building numpy array for pairs_labels length 8308185\n",
            "Saving numpy pairs_labels size (8308185, 1)\n",
            "Saving pickle locations size 343\n",
            "Saving pickle conll_tokens size 343\n",
            "Saving pickle spacy_lookup size 343\n",
            "Saving pickle doc size 343\n",
            "=> build_and_gather_multiple_arrays time elapsed 341.2252473831177\n",
            "🌋 Building tunable vocabulary matrix from static vocabulary\n",
            "🌋 Saving vocabulary\n",
            "🌋 Saving vocabulary\n",
            "Saving tunable voc, size: (34288, 50)\n",
            "Saving static voc, size: (103144, 50)\n",
            "=> save_vocabulary time elapsed 0.40743350982666016\n",
            "=> total time elapsed 374.79886198043823\n",
            "🌋 Building key file from corpus\n",
            "Saving in /content/drive/MyDrive/data/dev//key.txt\n",
            "In /content/drive/MyDrive/data/dev/\n",
            "In /content/drive/MyDrive/data/dev/numpy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m neuralcoref.train.conllparser --path /content/drive/MyDrive/data/test/"
      ],
      "metadata": {
        "id": "0yuj1xGhi6ID",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d08851d4-f21a-4ab7-8649-8a2009b1dccd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: spacy.vocab.Vocab size changed, may indicate binary incompatibility. Expected 104 from C header, got 112 from PyObject\n",
            "  return f(*args, **kwds)\n",
            "Loading embeddings from /content/neuralcoref/neuralcoref/train/weights/static_word\n",
            "Loading embeddings from /content/neuralcoref/neuralcoref/train/weights/tuned_word\n",
            "🌋 Reading files\n",
            "In /content/drive/MyDrive/data/test/ /content/drive/MyDrive/data/test\n",
            "In /content/drive/MyDrive/data/test/numpy /content/drive/MyDrive/data/test/numpy\n",
            "utts_text size 9479\n",
            "utts_tokens size 9479\n",
            "utts_corefs size 9479\n",
            "utts_speakers size 9479\n",
            "utts_doc_idx size 9479\n",
            "🌋 Building docs\n",
            "🌋 Loading spacy model\n",
            "\n",
            "\u001b[38;5;1m✘ Can't find model meta.json\u001b[0m\n",
            "/usr/local/lib/python3.7/site-packages/spacy/data/en_core_web_lg/meta.json\n",
            "\n",
            "Could not detect model en_core_web_lg\n",
            "\n",
            "\u001b[38;5;1m✘ Can't find model meta.json\u001b[0m\n",
            "/usr/local/lib/python3.7/site-packages/spacy/data/en_core_web_md/meta.json\n",
            "\n",
            "Could not detect model en_core_web_md\n",
            "\u001b[1m\n",
            "===================== Info about model 'en_core_web_sm' =====================\u001b[0m\n",
            "\n",
            "lang             en                            \n",
            "name             core_web_sm                   \n",
            "license          MIT                           \n",
            "author           Explosion                     \n",
            "url              https://explosion.ai          \n",
            "email            contact@explosion.ai          \n",
            "description      English multi-task CNN trained on OntoNotes. Assigns context-specific token vectors, POS tags, dependency parse and named entities.\n",
            "sources          [{'name': 'OntoNotes 5', 'url': 'https://catalog.ldc.upenn.edu/LDC2013T19', 'license': 'commercial (licensed by Explosion)'}]\n",
            "pipeline         ['tagger', 'parser', 'ner']   \n",
            "version          2.3.1                         \n",
            "spacy_version    >=2.3.0,<2.4.0                \n",
            "parent_package   spacy                         \n",
            "labels           {'tagger': ['$', \"''\", ',', '-LRB-', '-RRB-', '.', ':', 'ADD', 'AFX', 'CC', 'CD', 'DT', 'EX', 'FW', 'HYPH', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NFP', 'NN', 'NNP', 'NNPS', 'NNS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'SYM', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB', 'XX', '_SP', '``'], 'parser': ['ROOT', 'acl', 'acomp', 'advcl', 'advmod', 'agent', 'amod', 'appos', 'attr', 'aux', 'auxpass', 'case', 'cc', 'ccomp', 'compound', 'conj', 'csubj', 'csubjpass', 'dative', 'dep', 'det', 'dobj', 'expl', 'intj', 'mark', 'meta', 'neg', 'nmod', 'npadvmod', 'nsubj', 'nsubjpass', 'nummod', 'oprd', 'parataxis', 'pcomp', 'pobj', 'poss', 'preconj', 'predet', 'prep', 'prt', 'punct', 'quantmod', 'relcl', 'xcomp'], 'ner': ['CARDINAL', 'DATE', 'EVENT', 'FAC', 'GPE', 'LANGUAGE', 'LAW', 'LOC', 'MONEY', 'NORP', 'ORDINAL', 'ORG', 'PERCENT', 'PERSON', 'PRODUCT', 'QUANTITY', 'TIME', 'WORK_OF_ART']}\n",
            "source           /usr/local/lib/python3.7/site-packages/en_core_web_sm\n",
            "\n",
            "Loading model en_core_web_sm\n",
            "🌋 Parsing utterances and filling docs with use_gold_mentions=False\n",
            "9479it [00:34, 274.29it/s]\n",
            "=> read_corpus time elapsed 36.969921588897705\n",
            "🌋 Extracting mentions features with 1 job(s)\n",
            "100% 338/338 [00:35<00:00,  9.65it/s]\n",
            "🌋 Building and gathering array with 1 job(s)\n",
            "100% 338/338 [04:22<00:00,  1.29it/s]\n",
            "100% 348/348 [00:01<00:00, 307.42it/s]\n",
            "Building numpy array for mentions_features length 61827\n",
            "Saving numpy mentions_features size (61827, 6)\n",
            "Building numpy array for mentions_labels length 61827\n",
            "Saving numpy mentions_labels size (61827, 1)\n",
            "Building numpy array for mentions_pairs_length length 61827\n",
            "Saving numpy mentions_pairs_length size (61827, 1)\n",
            "Building numpy array for mentions_pairs_start_index length 61827\n",
            "Saving numpy mentions_pairs_start_index size (61827, 1)\n",
            "Building numpy array for mentions_spans length 61827\n",
            "Saving numpy mentions_spans size (61827, 250)\n",
            "Building numpy array for mentions_words length 61827\n",
            "Saving numpy mentions_words size (61827, 8)\n",
            "Building numpy array for pairs_ant_index length 8063486\n",
            "Saving numpy pairs_ant_index size (8063486, 1)\n",
            "Building numpy array for pairs_features length 8063486\n",
            "Saving numpy pairs_features size (8063486, 9)\n",
            "Building numpy array for pairs_labels length 8063486\n",
            "Saving numpy pairs_labels size (8063486, 1)\n",
            "Saving pickle locations size 348\n",
            "Saving pickle conll_tokens size 348\n",
            "Saving pickle spacy_lookup size 348\n",
            "Saving pickle doc size 348\n",
            "=> build_and_gather_multiple_arrays time elapsed 317.9240040779114\n",
            "🌋 Building tunable vocabulary matrix from static vocabulary\n",
            "🌋 Saving vocabulary\n",
            "🌋 Saving vocabulary\n",
            "Saving tunable voc, size: (34288, 50)\n",
            "Saving static voc, size: (103144, 50)\n",
            "=> save_vocabulary time elapsed 0.4097626209259033\n",
            "=> total time elapsed 355.3038401603699\n",
            "🌋 Building key file from corpus\n",
            "Saving in /content/drive/MyDrive/data/test//key.txt\n",
            "In /content/drive/MyDrive/data/test/\n",
            "In /content/drive/MyDrive/data/test/numpy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m neuralcoref.train.learn --train /content/drive/MyDrive/data/train/ --eval /content/drive/MyDrive/data/dev/"
      ],
      "metadata": {
        "id": "UCqeyZ2onvmU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f0f6b54-d8a7-4b2c-8495-50d8300fa375"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: spacy.vocab.Vocab size changed, may indicate binary incompatibility. Expected 104 from C header, got 112 from PyObject\n",
            "  return f(*args, **kwds)\n",
            "Namespace(all_pairs_epoch=200, all_pairs_l2=1e-06, all_pairs_lr=0.0002, batchsize=20000, checkpoint_file=None, conll_eval_interval=10, conll_train_interval=20, costfl=0.4, costfn=0.8, costs={'FN': 0.8, 'FL': 0.4, 'WL': 1.0}, costwl=1.0, cuda=True, eval='/content/drive/MyDrive/data/test//numpy/', evalkey='/content/drive/MyDrive/data/test//key.txt', h1=1000, h2=500, h3=500, lazy=True, log_interval=10, min_lr=2e-08, numworkers=8, on_eval_decrease='nothing', patience=3, ranking_epoch=200, ranking_l2=1e-05, ranking_lr=2e-06, save_path='/content/neuralcoref/neuralcoref/train/checkpoints/Apr28_21-16-33_0de2d24ecfff_', seed=1111, startstage=None, startstep=None, top_pairs_epoch=200, top_pairs_l2=1e-05, top_pairs_lr=0.0002, train='/content/drive/MyDrive/data/dev/numpy/', trainkey='/content/drive/MyDrive/data/dev/key.txt', weights=None)\n",
            "Training for 200 200 200 epochs\n",
            "loading /content/drive/MyDrive/data/dev/numpy/tuned_word_embeddings.npy\n",
            "torch.Size([34288, 50])\n",
            "loading /content/drive/MyDrive/data/dev/numpy/tuned_word_vocabulary.txt\n",
            "🏝 Loading Dataset at /content/drive/MyDrive/data/dev/numpy/\n",
            "Reading mentions_features.npy, mentions_labels.npy, mentions_pairs_length.npy, mentions_pairs_start_index.npy, mentions_spans.npy, mentions_words.npy, pairs_ant_index.npy, pairs_features.npy, pairs_labels.npy, tuned_word_embeddings.npy, static_word_embeddings.npy, 🏝 Loading Dataset at /content/drive/MyDrive/data/test//numpy/\n",
            "Reading mentions_features.npy, mentions_labels.npy, mentions_pairs_length.npy, mentions_pairs_start_index.npy, mentions_spans.npy, mentions_words.npy, pairs_ant_index.npy, pairs_features.npy, pairs_labels.npy, tuned_word_embeddings.npy, static_word_embeddings.npy, Vocabulary: 34288\n",
            "🏝 Build model\n",
            "🏝 Loading conll evaluator\n",
            "Preparing batches 📚\n",
            "Dataset has: 406 batches, 61827 mentions, 8063486 pairs\n",
            "Reading locations.bin, conll_tokens.bin, spacy_lookup.bin, doc.bin, Done\n",
            "Preparing batches 📚\n",
            "Dataset has: 417 batches, 59866 mentions, 8308185 pairs\n",
            "Reading locations.bin, conll_tokens.bin, spacy_lookup.bin, doc.bin, Done\n",
            "🏝 Testing evaluator and getting first eval score\n",
            "🌋 Test evaluator / print all mentions\n",
            "🌋 Building test file\n",
            "🌋 Construct test file\n",
            "Writing in /content/neuralcoref/neuralcoref/train/test_mentions.txt\n",
            "🌋 Computing score\n",
            "Mention identification recall 0.9061930783242259 <= Detected mentions 17910.0 True mentions 19764.0\n",
            "Scores {'muc': (0, 0.0, 0), 'bcub': (0.28970738098704324, 0.20003834914352914, 0.23666397749661924), 'ceafe': (0.037112078019588005, 0.5062457579984444, 0.06915454539354512)}\n",
            "F1_conll 0.10193950763005478\n",
            "🌋 Building test file\n",
            "🌋 Build coreference clusters\n",
            "🌋 Construct test file\n",
            "Writing in /content/neuralcoref/neuralcoref/train/test_corefs.txt\n",
            "🌋 Computing score\n",
            "Mention identification recall 0.7560210483707751 <= Detected mentions 14942.0 True mentions 19764.0\n",
            "Scores {'muc': (0.16766861566334515, 0.5346638655462185, 0.2552818005140744), 'bcub': (0.02674058195048437, 0.4420548158193048, 0.05043054213099261), 'ceafe': (0.1420188107635627, 0.0739550735661977, 0.09726186691874869)}\n",
            "F1_conll 0.1343247365212719\n",
            "|| s/evaluation 761.10\n",
            "🏝 Preparing dataloader\n",
            "Dataloader parameters: batchsize 20000 numworkers 8\n",
            "Preparing batches 📚\n",
            "Dataset has: 417 batches, 59866 mentions, 8308185 pairs\n",
            "🏝 Start training\n",
            "🚘 allpairs Epoch 0\n",
            "| epoch   0 |    10/  417 batches | lr 2.00e-04 | ms/batch 5160.19 | loss 1.57e-01\n",
            "| epoch   0 |    20/  417 batches | lr 2.00e-04 | ms/batch 1482.77 | loss 8.38e-02\n",
            "| epoch   0 |    30/  417 batches | lr 2.00e-04 | ms/batch 1573.04 | loss 1.35e-02\n",
            "| epoch   0 |    40/  417 batches | lr 2.00e-04 | ms/batch 1563.47 | loss 6.89e-02\n",
            "| epoch   0 |    50/  417 batches | lr 2.00e-04 | ms/batch 2839.35 | loss 5.45e-02\n",
            "| epoch   0 |    60/  417 batches | lr 2.00e-04 | ms/batch 1515.55 | loss 2.24e-02\n",
            "| epoch   0 |    70/  417 batches | lr 2.00e-04 | ms/batch 1503.74 | loss 3.48e-03\n",
            "| epoch   0 |    80/  417 batches | lr 2.00e-04 | ms/batch 1527.63 | loss 3.80e-02\n",
            "| epoch   0 |    90/  417 batches | lr 2.00e-04 | ms/batch 2956.11 | loss 6.03e-02\n",
            "| epoch   0 |   100/  417 batches | lr 2.00e-04 | ms/batch 1649.42 | loss 5.30e-02\n",
            "| epoch   0 |   110/  417 batches | lr 2.00e-04 | ms/batch 1414.52 | loss 2.70e-02\n",
            "| epoch   0 |   120/  417 batches | lr 2.00e-04 | ms/batch 1534.20 | loss 2.64e-02\n",
            "| epoch   0 |   130/  417 batches | lr 2.00e-04 | ms/batch 2868.57 | loss 5.88e-02\n",
            "| epoch   0 |   140/  417 batches | lr 2.00e-04 | ms/batch 1511.31 | loss 3.20e-02\n",
            "| epoch   0 |   150/  417 batches | lr 2.00e-04 | ms/batch 1536.24 | loss 4.69e-02\n",
            "| epoch   0 |   160/  417 batches | lr 2.00e-04 | ms/batch 1798.98 | loss 6.48e-02\n",
            "| epoch   0 |   170/  417 batches | lr 2.00e-04 | ms/batch 2582.63 | loss 5.02e-02\n",
            "| epoch   0 |   180/  417 batches | lr 2.00e-04 | ms/batch 1507.93 | loss 5.58e-02\n",
            "| epoch   0 |   190/  417 batches | lr 2.00e-04 | ms/batch 1500.40 | loss 9.21e-02\n",
            "| epoch   0 |   200/  417 batches | lr 2.00e-04 | ms/batch 1825.64 | loss 4.19e-02\n",
            "| epoch   0 |   210/  417 batches | lr 2.00e-04 | ms/batch 2533.30 | loss 2.46e-02\n",
            "| epoch   0 |   220/  417 batches | lr 2.00e-04 | ms/batch 1603.04 | loss 4.14e-02\n",
            "| epoch   0 |   230/  417 batches | lr 2.00e-04 | ms/batch 1485.94 | loss 4.30e-02\n",
            "| epoch   0 |   240/  417 batches | lr 2.00e-04 | ms/batch 1587.82 | loss 3.94e-02\n",
            "| epoch   0 |   250/  417 batches | lr 2.00e-04 | ms/batch 2724.32 | loss 4.15e-02\n",
            "| epoch   0 |   260/  417 batches | lr 2.00e-04 | ms/batch 1555.09 | loss 6.05e-03\n",
            "| epoch   0 |   270/  417 batches | lr 2.00e-04 | ms/batch 1514.30 | loss 5.99e-02\n",
            "| epoch   0 |   280/  417 batches | lr 2.00e-04 | ms/batch 1559.27 | loss 4.95e-02\n",
            "| epoch   0 |   290/  417 batches | lr 2.00e-04 | ms/batch 3025.53 | loss 5.58e-02\n",
            "| epoch   0 |   300/  417 batches | lr 2.00e-04 | ms/batch 1509.35 | loss 1.44e-02\n",
            "| epoch   0 |   310/  417 batches | lr 2.00e-04 | ms/batch 1534.81 | loss 4.60e-02\n",
            "| epoch   0 |   320/  417 batches | lr 2.00e-04 | ms/batch 1595.74 | loss 1.15e-03\n",
            "| epoch   0 |   330/  417 batches | lr 2.00e-04 | ms/batch 2767.19 | loss 6.43e-02\n",
            "| epoch   0 |   340/  417 batches | lr 2.00e-04 | ms/batch 1577.89 | loss 5.29e-02\n",
            "| epoch   0 |   350/  417 batches | lr 2.00e-04 | ms/batch 1561.90 | loss 4.00e-02\n",
            "| epoch   0 |   360/  417 batches | lr 2.00e-04 | ms/batch 1618.68 | loss 3.55e-02\n",
            "| epoch   0 |   370/  417 batches | lr 2.00e-04 | ms/batch 2767.39 | loss 1.55e-02\n",
            "| epoch   0 |   380/  417 batches | lr 2.00e-04 | ms/batch 1629.51 | loss 3.92e-02\n",
            "| epoch   0 |   390/  417 batches | lr 2.00e-04 | ms/batch 1539.75 | loss 6.42e-02\n",
            "| epoch   0 |   400/  417 batches | lr 2.00e-04 | ms/batch 1475.34 | loss 5.70e-03\n",
            "| epoch   0 |   410/  417 batches | lr 2.00e-04 | ms/batch 1694.24 | loss 3.54e-02\n",
            "|| min/epoch 13.14 | est. remaining time (h) 43.80 | loss 1.72e+01\n",
            "🌋 Building test file\n",
            "🌋 Build coreference clusters\n",
            "🌋 Construct test file\n",
            "Writing in /content/neuralcoref/neuralcoref/train/test_corefs.txt\n",
            "🌋 Computing score\n",
            "Mention identification recall 0.5061341686243801 <= Detected mentions 9695.0 True mentions 19155.0\n",
            "Scores {'muc': (0.6452934451219512, 0.46358658453114304, 0.5395522982553971), 'bcub': (0.3790565518487315, 0.35028314233914754, 0.3641022726828634), 'ceafe': (0.45021132937955144, 0.158950546338359, 0.23495014913158185)}\n",
            "F1_conll 0.37953490668994744\n",
            "|| min/train evaluation 13.14 | F1_conll  0.38\n",
            "🌋 Building test file\n",
            "🌋 Build coreference clusters\n",
            "🌋 Construct test file\n",
            "Writing in /content/neuralcoref/neuralcoref/train/test_corefs.txt\n",
            "🌋 Computing score\n",
            "Mention identification recall 0.5007589556769885 <= Detected mentions 9897.0 True mentions 19764.0\n",
            "Scores {'muc': (0.645487160120846, 0.4488576680672269, 0.529507434944238), 'bcub': (0.3660505523396302, 0.31850372703818913, 0.3406259188402865), 'ceafe': (0.40667475722956525, 0.1512916241591013, 0.2205383212251679)}\n",
            "F1_conll 0.3635572250032308\n",
            "|| min/evaluation 13.14\n",
            "🚘 allpairs Epoch 1\n",
            "| epoch   1 |    10/  417 batches | lr 2.00e-04 | ms/batch 3092.43 | loss 3.81e-02\n",
            "| epoch   1 |    20/  417 batches | lr 2.00e-04 | ms/batch 1503.55 | loss 3.48e-02\n",
            "| epoch   1 |    30/  417 batches | lr 2.00e-04 | ms/batch 1585.20 | loss 5.25e-02\n",
            "| epoch   1 |    40/  417 batches | lr 2.00e-04 | ms/batch 2665.16 | loss 2.11e-02\n",
            "| epoch   1 |    50/  417 batches | lr 2.00e-04 | ms/batch 1717.11 | loss 8.88e-03\n",
            "| epoch   1 |    60/  417 batches | lr 2.00e-04 | ms/batch 1508.92 | loss 1.65e-03\n",
            "| epoch   1 |    70/  417 batches | lr 2.00e-04 | ms/batch 1971.79 | loss 3.26e-02\n",
            "| epoch   1 |    80/  417 batches | lr 2.00e-04 | ms/batch 1564.48 | loss 8.03e-03\n",
            "| epoch   1 |    90/  417 batches | lr 2.00e-04 | ms/batch 2233.58 | loss 3.10e-03\n",
            "| epoch   1 |   100/  417 batches | lr 2.00e-04 | ms/batch 1553.98 | loss 4.72e-02\n",
            "| epoch   1 |   110/  417 batches | lr 2.00e-04 | ms/batch 2140.48 | loss 3.41e-02\n",
            "| epoch   1 |   120/  417 batches | lr 2.00e-04 | ms/batch 1546.68 | loss 7.27e-02\n",
            "| epoch   1 |   130/  417 batches | lr 2.00e-04 | ms/batch 2139.56 | loss 3.18e-02\n",
            "| epoch   1 |   140/  417 batches | lr 2.00e-04 | ms/batch 2858.68 | loss 2.37e-02\n",
            "| epoch   1 |   150/  417 batches | lr 2.00e-04 | ms/batch 1545.64 | loss 3.18e-02\n",
            "| epoch   1 |   160/  417 batches | lr 2.00e-04 | ms/batch 1496.81 | loss 2.41e-02\n",
            "| epoch   1 |   170/  417 batches | lr 2.00e-04 | ms/batch 1556.00 | loss 5.72e-02\n",
            "| epoch   1 |   180/  417 batches | lr 2.00e-04 | ms/batch 2778.40 | loss 2.00e-02\n",
            "| epoch   1 |   190/  417 batches | lr 2.00e-04 | ms/batch 1573.52 | loss 3.70e-02\n",
            "| epoch   1 |   200/  417 batches | lr 2.00e-04 | ms/batch 1559.38 | loss 2.79e-02\n",
            "| epoch   1 |   210/  417 batches | lr 2.00e-04 | ms/batch 1533.20 | loss 3.10e-02\n",
            "| epoch   1 |   220/  417 batches | lr 2.00e-04 | ms/batch 2912.41 | loss 2.45e-02\n",
            "| epoch   1 |   230/  417 batches | lr 2.00e-04 | ms/batch 1610.11 | loss 2.13e-02\n",
            "| epoch   1 |   240/  417 batches | lr 2.00e-04 | ms/batch 1521.54 | loss 5.13e-02\n",
            "| epoch   1 |   250/  417 batches | lr 2.00e-04 | ms/batch 1426.84 | loss 4.23e-02\n",
            "| epoch   1 |   260/  417 batches | lr 2.00e-04 | ms/batch 2822.37 | loss 2.58e-02\n",
            "| epoch   1 |   270/  417 batches | lr 2.00e-04 | ms/batch 1543.74 | loss 4.49e-02\n",
            "| epoch   1 |   280/  417 batches | lr 2.00e-04 | ms/batch 1589.09 | loss 2.31e-02\n",
            "| epoch   1 |   290/  417 batches | lr 2.00e-04 | ms/batch 1463.55 | loss 3.70e-02\n",
            "| epoch   1 |   300/  417 batches | lr 2.00e-04 | ms/batch 2761.52 | loss 3.11e-02\n",
            "| epoch   1 |   310/  417 batches | lr 2.00e-04 | ms/batch 1502.20 | loss 5.96e-02\n",
            "| epoch   1 |   320/  417 batches | lr 2.00e-04 | ms/batch 1566.67 | loss 6.90e-03\n",
            "| epoch   1 |   330/  417 batches | lr 2.00e-04 | ms/batch 1829.15 | loss 7.17e-02\n",
            "| epoch   1 |   340/  417 batches | lr 2.00e-04 | ms/batch 2555.14 | loss 2.78e-02\n",
            "| epoch   1 |   350/  417 batches | lr 2.00e-04 | ms/batch 1498.80 | loss 3.96e-02\n",
            "| epoch   1 |   360/  417 batches | lr 2.00e-04 | ms/batch 1515.41 | loss 2.93e-02\n",
            "| epoch   1 |   370/  417 batches | lr 2.00e-04 | ms/batch 1753.53 | loss 2.34e-02\n",
            "| epoch   1 |   380/  417 batches | lr 2.00e-04 | ms/batch 2679.90 | loss 1.66e-02\n",
            "| epoch   1 |   390/  417 batches | lr 2.00e-04 | ms/batch 1519.09 | loss 2.71e-02\n",
            "| epoch   1 |   400/  417 batches | lr 2.00e-04 | ms/batch 1583.24 | loss 3.99e-02\n",
            "| epoch   1 |   410/  417 batches | lr 2.00e-04 | ms/batch 1371.37 | loss 4.14e-02\n",
            "|| min/epoch 12.90 | est. remaining time (h) 85.14 | loss 1.30e+01\n",
            "🚘 allpairs Epoch 2\n",
            "| epoch   2 |    10/  417 batches | lr 2.00e-04 | ms/batch 3091.35 | loss 4.38e-02\n",
            "| epoch   2 |    20/  417 batches | lr 2.00e-04 | ms/batch 1897.92 | loss 3.71e-02\n",
            "| epoch   2 |    30/  417 batches | lr 2.00e-04 | ms/batch 1502.58 | loss 6.29e-03\n",
            "| epoch   2 |    40/  417 batches | lr 2.00e-04 | ms/batch 2348.99 | loss 2.83e-02\n",
            "| epoch   2 |    50/  417 batches | lr 2.00e-04 | ms/batch 1673.09 | loss 4.67e-02\n",
            "| epoch   2 |    60/  417 batches | lr 2.00e-04 | ms/batch 1802.05 | loss 3.01e-02\n",
            "| epoch   2 |    70/  417 batches | lr 2.00e-04 | ms/batch 1475.05 | loss 3.42e-02\n",
            "| epoch   2 |    80/  417 batches | lr 2.00e-04 | ms/batch 2523.57 | loss 2.99e-03\n",
            "| epoch   2 |    90/  417 batches | lr 2.00e-04 | ms/batch 1550.56 | loss 2.93e-02\n",
            "| epoch   2 |   100/  417 batches | lr 2.00e-04 | ms/batch 1800.54 | loss 4.93e-02\n",
            "| epoch   2 |   110/  417 batches | lr 2.00e-04 | ms/batch 1594.75 | loss 2.21e-02\n",
            "| epoch   2 |   120/  417 batches | lr 2.00e-04 | ms/batch 2468.44 | loss 2.08e-02\n",
            "| epoch   2 |   130/  417 batches | lr 2.00e-04 | ms/batch 1530.45 | loss 2.43e-02\n",
            "| epoch   2 |   140/  417 batches | lr 2.00e-04 | ms/batch 1953.37 | loss 8.21e-03\n",
            "| epoch   2 |   150/  417 batches | lr 2.00e-04 | ms/batch 1493.77 | loss 1.83e-02\n",
            "| epoch   2 |   160/  417 batches | lr 2.00e-04 | ms/batch 2289.18 | loss 2.31e-02\n",
            "| epoch   2 |   170/  417 batches | lr 2.00e-04 | ms/batch 1785.06 | loss 4.61e-02\n",
            "| epoch   2 |   180/  417 batches | lr 2.00e-04 | ms/batch 1795.06 | loss 2.05e-02\n",
            "| epoch   2 |   190/  417 batches | lr 2.00e-04 | ms/batch 1522.51 | loss 2.12e-02\n",
            "| epoch   2 |   200/  417 batches | lr 2.00e-04 | ms/batch 2576.58 | loss 4.28e-02\n",
            "| epoch   2 |   210/  417 batches | lr 2.00e-04 | ms/batch 1521.82 | loss 3.83e-03\n",
            "| epoch   2 |   220/  417 batches | lr 2.00e-04 | ms/batch 1961.61 | loss 1.67e-02\n",
            "| epoch   2 |   230/  417 batches | lr 2.00e-04 | ms/batch 1923.78 | loss 2.22e-02\n",
            "| epoch   2 |   240/  417 batches | lr 2.00e-04 | ms/batch 1932.46 | loss 2.86e-02\n",
            "| epoch   2 |   250/  417 batches | lr 2.00e-04 | ms/batch 1499.01 | loss 5.31e-02\n",
            "| epoch   2 |   260/  417 batches | lr 2.00e-04 | ms/batch 1990.63 | loss 3.42e-02\n",
            "| epoch   2 |   270/  417 batches | lr 2.00e-04 | ms/batch 1934.31 | loss 3.81e-02\n",
            "| epoch   2 |   280/  417 batches | lr 2.00e-04 | ms/batch 1831.56 | loss 2.37e-02\n",
            "| epoch   2 |   290/  417 batches | lr 2.00e-04 | ms/batch 1367.63 | loss 4.54e-02\n",
            "| epoch   2 |   300/  417 batches | lr 2.00e-04 | ms/batch 1978.84 | loss 6.69e-03\n",
            "| epoch   2 |   310/  417 batches | lr 2.00e-04 | ms/batch 2073.45 | loss 2.17e-02\n",
            "| epoch   2 |   320/  417 batches | lr 2.00e-04 | ms/batch 1939.41 | loss 1.95e-02\n",
            "| epoch   2 |   330/  417 batches | lr 2.00e-04 | ms/batch 1542.44 | loss 3.82e-02\n",
            "| epoch   2 |   340/  417 batches | lr 2.00e-04 | ms/batch 1880.31 | loss 3.92e-02\n",
            "| epoch   2 |   350/  417 batches | lr 2.00e-04 | ms/batch 2060.81 | loss 2.95e-02\n",
            "| epoch   2 |   360/  417 batches | lr 2.00e-04 | ms/batch 2063.23 | loss 2.98e-02\n",
            "| epoch   2 |   370/  417 batches | lr 2.00e-04 | ms/batch 1565.73 | loss 2.80e-02\n",
            "| epoch   2 |   380/  417 batches | lr 2.00e-04 | ms/batch 2043.61 | loss 2.55e-02\n",
            "| epoch   2 |   390/  417 batches | lr 2.00e-04 | ms/batch 2217.14 | loss 9.49e-04\n",
            "| epoch   2 |   400/  417 batches | lr 2.00e-04 | ms/batch 1530.21 | loss 2.05e-02\n",
            "| epoch   2 |   410/  417 batches | lr 2.00e-04 | ms/batch 1333.76 | loss 4.50e-02\n",
            "|| min/epoch 12.90 | est. remaining time (h) 70.66 | loss 1.18e+01\n",
            "🚘 allpairs Epoch 3\n",
            "| epoch   3 |    10/  417 batches | lr 2.00e-04 | ms/batch 3063.54 | loss 2.96e-02\n",
            "| epoch   3 |    20/  417 batches | lr 2.00e-04 | ms/batch 1765.14 | loss 2.69e-02\n",
            "| epoch   3 |    30/  417 batches | lr 2.00e-04 | ms/batch 2819.46 | loss 1.42e-02\n",
            "| epoch   3 |    40/  417 batches | lr 2.00e-04 | ms/batch 1514.81 | loss 3.14e-02\n",
            "| epoch   3 |    50/  417 batches | lr 2.00e-04 | ms/batch 1633.79 | loss 4.42e-02\n",
            "| epoch   3 |    60/  417 batches | lr 2.00e-04 | ms/batch 1511.87 | loss 3.36e-02\n",
            "| epoch   3 |    70/  417 batches | lr 2.00e-04 | ms/batch 2826.12 | loss 1.32e-02\n",
            "| epoch   3 |    80/  417 batches | lr 2.00e-04 | ms/batch 1492.51 | loss 7.32e-03\n",
            "| epoch   3 |    90/  417 batches | lr 2.00e-04 | ms/batch 1666.79 | loss 3.50e-02\n",
            "| epoch   3 |   100/  417 batches | lr 2.00e-04 | ms/batch 1777.26 | loss 4.99e-03\n",
            "| epoch   3 |   110/  417 batches | lr 2.00e-04 | ms/batch 2523.67 | loss 3.84e-02\n",
            "| epoch   3 |   120/  417 batches | lr 2.00e-04 | ms/batch 1494.28 | loss 1.70e-02\n",
            "| epoch   3 |   130/  417 batches | lr 2.00e-04 | ms/batch 1529.39 | loss 2.19e-02\n",
            "| epoch   3 |   140/  417 batches | lr 2.00e-04 | ms/batch 2021.58 | loss 3.81e-02\n",
            "| epoch   3 |   150/  417 batches | lr 2.00e-04 | ms/batch 2264.98 | loss 2.59e-02\n",
            "| epoch   3 |   160/  417 batches | lr 2.00e-04 | ms/batch 1482.87 | loss 1.49e-02\n",
            "| epoch   3 |   170/  417 batches | lr 2.00e-04 | ms/batch 1654.71 | loss 2.77e-02\n",
            "| epoch   3 |   180/  417 batches | lr 2.00e-04 | ms/batch 2143.79 | loss 3.23e-02\n",
            "| epoch   3 |   190/  417 batches | lr 2.00e-04 | ms/batch 2092.32 | loss 2.36e-02\n",
            "| epoch   3 |   200/  417 batches | lr 2.00e-04 | ms/batch 1495.86 | loss 2.65e-02\n",
            "| epoch   3 |   210/  417 batches | lr 2.00e-04 | ms/batch 1685.90 | loss 1.80e-02\n",
            "| epoch   3 |   220/  417 batches | lr 2.00e-04 | ms/batch 2184.15 | loss 7.00e-04\n",
            "| epoch   3 |   230/  417 batches | lr 2.00e-04 | ms/batch 1987.06 | loss 1.24e-02\n",
            "| epoch   3 |   240/  417 batches | lr 2.00e-04 | ms/batch 1498.62 | loss 2.01e-02\n",
            "| epoch   3 |   250/  417 batches | lr 2.00e-04 | ms/batch 1675.08 | loss 1.87e-02\n",
            "| epoch   3 |   260/  417 batches | lr 2.00e-04 | ms/batch 2343.80 | loss 1.98e-02\n",
            "| epoch   3 |   270/  417 batches | lr 2.00e-04 | ms/batch 2005.57 | loss 3.15e-02\n",
            "| epoch   3 |   280/  417 batches | lr 2.00e-04 | ms/batch 1508.80 | loss 2.99e-02\n",
            "| epoch   3 |   290/  417 batches | lr 2.00e-04 | ms/batch 2039.11 | loss 2.17e-02\n",
            "| epoch   3 |   300/  417 batches | lr 2.00e-04 | ms/batch 2126.81 | loss 3.64e-02\n",
            "| epoch   3 |   310/  417 batches | lr 2.00e-04 | ms/batch 1689.34 | loss 4.26e-02\n",
            "| epoch   3 |   320/  417 batches | lr 2.00e-04 | ms/batch 1552.93 | loss 6.92e-03\n",
            "| epoch   3 |   330/  417 batches | lr 2.00e-04 | ms/batch 2097.86 | loss 2.54e-02\n",
            "| epoch   3 |   340/  417 batches | lr 2.00e-04 | ms/batch 2219.34 | loss 3.03e-02\n",
            "| epoch   3 |   350/  417 batches | lr 2.00e-04 | ms/batch 1601.71 | loss 2.01e-02\n",
            "| epoch   3 |   360/  417 batches | lr 2.00e-04 | ms/batch 1506.65 | loss 7.71e-02\n",
            "| epoch   3 |   370/  417 batches | lr 2.00e-04 | ms/batch 2148.89 | loss 3.35e-02\n",
            "| epoch   3 |   380/  417 batches | lr 2.00e-04 | ms/batch 2142.86 | loss 2.72e-02\n",
            "| epoch   3 |   390/  417 batches | lr 2.00e-04 | ms/batch 1570.20 | loss 1.28e-02\n",
            "| epoch   3 |   400/  417 batches | lr 2.00e-04 | ms/batch 1492.97 | loss 3.84e-02\n",
            "| epoch   3 |   410/  417 batches | lr 2.00e-04 | ms/batch 1749.72 | loss 1.64e-02\n",
            "|| min/epoch 12.99 | est. remaining time (h) 63.39 | loss 1.09e+01\n",
            "🚘 allpairs Epoch 4\n",
            "| epoch   4 |    10/  417 batches | lr 2.00e-04 | ms/batch 3664.21 | loss 2.92e-02\n",
            "| epoch   4 |    20/  417 batches | lr 2.00e-04 | ms/batch 1487.49 | loss 3.28e-02\n",
            "| epoch   4 |    30/  417 batches | lr 2.00e-04 | ms/batch 1521.42 | loss 1.21e-02\n",
            "| epoch   4 |    40/  417 batches | lr 2.00e-04 | ms/batch 2476.77 | loss 3.49e-02\n",
            "| epoch   4 |    50/  417 batches | lr 2.00e-04 | ms/batch 1980.68 | loss 1.64e-02\n",
            "| epoch   4 |    60/  417 batches | lr 2.00e-04 | ms/batch 1464.55 | loss 2.24e-02\n",
            "| epoch   4 |    70/  417 batches | lr 2.00e-04 | ms/batch 1533.01 | loss 1.44e-02\n",
            "| epoch   4 |    80/  417 batches | lr 2.00e-04 | ms/batch 2789.92 | loss 3.43e-02\n",
            "| epoch   4 |    90/  417 batches | lr 2.00e-04 | ms/batch 1566.79 | loss 1.53e-02\n",
            "| epoch   4 |   100/  417 batches | lr 2.00e-04 | ms/batch 1496.41 | loss 3.85e-02\n",
            "| epoch   4 |   110/  417 batches | lr 2.00e-04 | ms/batch 1558.68 | loss 4.63e-02\n",
            "| epoch   4 |   120/  417 batches | lr 2.00e-04 | ms/batch 2861.46 | loss 1.56e-02\n",
            "| epoch   4 |   130/  417 batches | lr 2.00e-04 | ms/batch 1661.51 | loss 2.42e-02\n",
            "| epoch   4 |   140/  417 batches | lr 2.00e-04 | ms/batch 1561.70 | loss 2.10e-02\n",
            "| epoch   4 |   150/  417 batches | lr 2.00e-04 | ms/batch 1599.49 | loss 2.91e-02\n",
            "| epoch   4 |   160/  417 batches | lr 2.00e-04 | ms/batch 2784.82 | loss 3.58e-02\n",
            "| epoch   4 |   170/  417 batches | lr 2.00e-04 | ms/batch 1477.49 | loss 3.38e-02\n",
            "| epoch   4 |   180/  417 batches | lr 2.00e-04 | ms/batch 1516.21 | loss 7.74e-03\n",
            "| epoch   4 |   190/  417 batches | lr 2.00e-04 | ms/batch 1681.00 | loss 2.92e-03\n",
            "| epoch   4 |   200/  417 batches | lr 2.00e-04 | ms/batch 2909.13 | loss 1.98e-03\n",
            "| epoch   4 |   210/  417 batches | lr 2.00e-04 | ms/batch 1466.43 | loss 2.69e-02\n",
            "| epoch   4 |   220/  417 batches | lr 2.00e-04 | ms/batch 1515.89 | loss 3.50e-03\n",
            "| epoch   4 |   230/  417 batches | lr 2.00e-04 | ms/batch 1594.74 | loss 2.82e-02\n",
            "| epoch   4 |   240/  417 batches | lr 2.00e-04 | ms/batch 2841.49 | loss 1.59e-02\n",
            "| epoch   4 |   250/  417 batches | lr 2.00e-04 | ms/batch 1531.86 | loss 4.50e-02\n",
            "| epoch   4 |   260/  417 batches | lr 2.00e-04 | ms/batch 1493.20 | loss 5.57e-02\n",
            "| epoch   4 |   270/  417 batches | lr 2.00e-04 | ms/batch 1534.02 | loss 3.23e-02\n",
            "| epoch   4 |   280/  417 batches | lr 2.00e-04 | ms/batch 2841.59 | loss 2.63e-02\n",
            "| epoch   4 |   290/  417 batches | lr 2.00e-04 | ms/batch 1495.00 | loss 4.16e-02\n",
            "| epoch   4 |   300/  417 batches | lr 2.00e-04 | ms/batch 1624.48 | loss 3.74e-02\n",
            "| epoch   4 |   310/  417 batches | lr 2.00e-04 | ms/batch 1486.23 | loss 3.47e-02\n",
            "| epoch   4 |   320/  417 batches | lr 2.00e-04 | ms/batch 2906.35 | loss 3.97e-02\n",
            "| epoch   4 |   330/  417 batches | lr 2.00e-04 | ms/batch 1540.97 | loss 8.11e-04\n",
            "| epoch   4 |   340/  417 batches | lr 2.00e-04 | ms/batch 1519.08 | loss 3.57e-02\n",
            "| epoch   4 |   350/  417 batches | lr 2.00e-04 | ms/batch 1525.39 | loss 2.51e-02\n",
            "| epoch   4 |   360/  417 batches | lr 2.00e-04 | ms/batch 2819.56 | loss 2.02e-02\n",
            "| epoch   4 |   370/  417 batches | lr 2.00e-04 | ms/batch 1571.10 | loss 3.75e-03\n",
            "| epoch   4 |   380/  417 batches | lr 2.00e-04 | ms/batch 1575.55 | loss 8.27e-03\n",
            "| epoch   4 |   390/  417 batches | lr 2.00e-04 | ms/batch 1511.22 | loss 7.53e-03\n",
            "| epoch   4 |   400/  417 batches | lr 2.00e-04 | ms/batch 2771.99 | loss 3.33e-02\n",
            "| epoch   4 |   410/  417 batches | lr 2.00e-04 | ms/batch 698.16 | loss 2.87e-02\n",
            "|| min/epoch 12.95 | est. remaining time (h) 58.92 | loss 1.03e+01\n",
            "🚘 allpairs Epoch 5\n",
            "| epoch   5 |    10/  417 batches | lr 2.00e-04 | ms/batch 3182.28 | loss 1.94e-02\n",
            "| epoch   5 |    20/  417 batches | lr 2.00e-04 | ms/batch 1597.89 | loss 2.93e-02\n",
            "| epoch   5 |    30/  417 batches | lr 2.00e-04 | ms/batch 1482.08 | loss 3.17e-02\n",
            "| epoch   5 |    40/  417 batches | lr 2.00e-04 | ms/batch 2804.36 | loss 2.71e-02\n",
            "| epoch   5 |    50/  417 batches | lr 2.00e-04 | ms/batch 1629.11 | loss 5.60e-02\n",
            "| epoch   5 |    60/  417 batches | lr 2.00e-04 | ms/batch 1466.78 | loss 1.69e-02\n",
            "| epoch   5 |    70/  417 batches | lr 2.00e-04 | ms/batch 1468.40 | loss 1.69e-02\n",
            "| epoch   5 |    80/  417 batches | lr 2.00e-04 | ms/batch 2720.14 | loss 2.45e-02\n",
            "| epoch   5 |    90/  417 batches | lr 2.00e-04 | ms/batch 1509.56 | loss 3.37e-02\n",
            "| epoch   5 |   100/  417 batches | lr 2.00e-04 | ms/batch 1473.89 | loss 8.73e-03\n",
            "| epoch   5 |   110/  417 batches | lr 2.00e-04 | ms/batch 1727.73 | loss 2.84e-02\n",
            "| epoch   5 |   120/  417 batches | lr 2.00e-04 | ms/batch 2724.32 | loss 3.21e-02\n",
            "| epoch   5 |   130/  417 batches | lr 2.00e-04 | ms/batch 1507.09 | loss 2.77e-02\n",
            "| epoch   5 |   140/  417 batches | lr 2.00e-04 | ms/batch 1573.75 | loss 2.91e-02\n",
            "| epoch   5 |   150/  417 batches | lr 2.00e-04 | ms/batch 1676.90 | loss 2.98e-02\n",
            "| epoch   5 |   160/  417 batches | lr 2.00e-04 | ms/batch 2783.82 | loss 2.88e-02\n",
            "| epoch   5 |   170/  417 batches | lr 2.00e-04 | ms/batch 1502.70 | loss 3.14e-02\n",
            "| epoch   5 |   180/  417 batches | lr 2.00e-04 | ms/batch 1571.98 | loss 2.95e-02\n",
            "| epoch   5 |   190/  417 batches | lr 2.00e-04 | ms/batch 1589.30 | loss 2.46e-02\n",
            "| epoch   5 |   200/  417 batches | lr 2.00e-04 | ms/batch 2904.02 | loss 4.33e-02\n",
            "| epoch   5 |   210/  417 batches | lr 2.00e-04 | ms/batch 1510.19 | loss 3.38e-02\n",
            "| epoch   5 |   220/  417 batches | lr 2.00e-04 | ms/batch 1557.66 | loss 3.27e-02\n",
            "| epoch   5 |   230/  417 batches | lr 2.00e-04 | ms/batch 2032.60 | loss 2.44e-02\n",
            "| epoch   5 |   240/  417 batches | lr 2.00e-04 | ms/batch 2528.28 | loss 3.12e-02\n",
            "| epoch   5 |   250/  417 batches | lr 2.00e-04 | ms/batch 1624.01 | loss 1.79e-02\n",
            "| epoch   5 |   260/  417 batches | lr 2.00e-04 | ms/batch 1739.38 | loss 1.02e-02\n",
            "| epoch   5 |   270/  417 batches | lr 2.00e-04 | ms/batch 1491.72 | loss 8.28e-03\n",
            "| epoch   5 |   280/  417 batches | lr 2.00e-04 | ms/batch 2533.40 | loss 1.77e-02\n",
            "| epoch   5 |   290/  417 batches | lr 2.00e-04 | ms/batch 1497.92 | loss 3.09e-02\n",
            "| epoch   5 |   300/  417 batches | lr 2.00e-04 | ms/batch 1877.79 | loss 2.31e-02\n",
            "| epoch   5 |   310/  417 batches | lr 2.00e-04 | ms/batch 1583.70 | loss 2.56e-02\n",
            "| epoch   5 |   320/  417 batches | lr 2.00e-04 | ms/batch 2972.02 | loss 3.76e-02\n",
            "| epoch   5 |   330/  417 batches | lr 2.00e-04 | ms/batch 1472.37 | loss 1.80e-02\n",
            "| epoch   5 |   340/  417 batches | lr 2.00e-04 | ms/batch 1648.27 | loss 2.05e-02\n",
            "| epoch   5 |   350/  417 batches | lr 2.00e-04 | ms/batch 1452.98 | loss 3.91e-03\n",
            "| epoch   5 |   360/  417 batches | lr 2.00e-04 | ms/batch 2610.19 | loss 3.17e-02\n",
            "| epoch   5 |   370/  417 batches | lr 2.00e-04 | ms/batch 1495.30 | loss 1.98e-02\n",
            "| epoch   5 |   380/  417 batches | lr 2.00e-04 | ms/batch 1504.52 | loss 1.87e-02\n",
            "| epoch   5 |   390/  417 batches | lr 2.00e-04 | ms/batch 1576.31 | loss 1.52e-02\n",
            "| epoch   5 |   400/  417 batches | lr 2.00e-04 | ms/batch 2849.73 | loss 3.57e-02\n",
            "| epoch   5 |   410/  417 batches | lr 2.00e-04 | ms/batch 849.79 | loss 7.17e-03\n",
            "|| min/epoch 12.93 | est. remaining time (h) 55.85 | loss 9.67e+00\n",
            "🚘 allpairs Epoch 6\n",
            "| epoch   6 |    10/  417 batches | lr 2.00e-04 | ms/batch 3311.36 | loss 1.15e-02\n",
            "| epoch   6 |    20/  417 batches | lr 2.00e-04 | ms/batch 1554.90 | loss 2.61e-02\n",
            "| epoch   6 |    30/  417 batches | lr 2.00e-04 | ms/batch 2850.62 | loss 1.98e-02\n",
            "| epoch   6 |    40/  417 batches | lr 2.00e-04 | ms/batch 1558.00 | loss 1.29e-02\n",
            "| epoch   6 |    50/  417 batches | lr 2.00e-04 | ms/batch 1525.75 | loss 1.53e-02\n",
            "| epoch   6 |    60/  417 batches | lr 2.00e-04 | ms/batch 1574.86 | loss 2.81e-02\n",
            "| epoch   6 |    70/  417 batches | lr 2.00e-04 | ms/batch 2729.14 | loss 1.77e-02\n",
            "| epoch   6 |    80/  417 batches | lr 2.00e-04 | ms/batch 1502.91 | loss 2.60e-02\n",
            "| epoch   6 |    90/  417 batches | lr 2.00e-04 | ms/batch 1714.80 | loss 1.59e-02\n",
            "| epoch   6 |   100/  417 batches | lr 2.00e-04 | ms/batch 1643.51 | loss 3.72e-02\n",
            "| epoch   6 |   110/  417 batches | lr 2.00e-04 | ms/batch 2713.74 | loss 1.94e-02\n",
            "| epoch   6 |   120/  417 batches | lr 2.00e-04 | ms/batch 1499.46 | loss 2.43e-02\n",
            "| epoch   6 |   130/  417 batches | lr 2.00e-04 | ms/batch 2285.46 | loss 2.00e-02\n",
            "| epoch   6 |   140/  417 batches | lr 2.00e-04 | ms/batch 1539.46 | loss 6.19e-03\n",
            "| epoch   6 |   150/  417 batches | lr 2.00e-04 | ms/batch 2055.40 | loss 1.59e-02\n",
            "| epoch   6 |   160/  417 batches | lr 2.00e-04 | ms/batch 1542.93 | loss 1.73e-02\n",
            "| epoch   6 |   170/  417 batches | lr 2.00e-04 | ms/batch 2394.74 | loss 1.28e-02\n",
            "| epoch   6 |   180/  417 batches | lr 2.00e-04 | ms/batch 1510.53 | loss 2.57e-03\n",
            "| epoch   6 |   190/  417 batches | lr 2.00e-04 | ms/batch 2065.08 | loss 1.75e-03\n",
            "| epoch   6 |   200/  417 batches | lr 2.00e-04 | ms/batch 1558.12 | loss 3.55e-02\n",
            "| epoch   6 |   210/  417 batches | lr 2.00e-04 | ms/batch 2200.61 | loss 1.71e-03\n",
            "| epoch   6 |   220/  417 batches | lr 2.00e-04 | ms/batch 1499.88 | loss 4.82e-02\n",
            "| epoch   6 |   230/  417 batches | lr 2.00e-04 | ms/batch 2202.56 | loss 2.26e-02\n",
            "| epoch   6 |   240/  417 batches | lr 2.00e-04 | ms/batch 1497.61 | loss 2.79e-02\n",
            "| epoch   6 |   250/  417 batches | lr 2.00e-04 | ms/batch 2353.19 | loss 2.57e-02\n",
            "| epoch   6 |   260/  417 batches | lr 2.00e-04 | ms/batch 1502.80 | loss 7.31e-03\n",
            "| epoch   6 |   270/  417 batches | lr 2.00e-04 | ms/batch 2006.77 | loss 3.47e-02\n",
            "| epoch   6 |   280/  417 batches | lr 2.00e-04 | ms/batch 1432.70 | loss 1.25e-02\n",
            "| epoch   6 |   290/  417 batches | lr 2.00e-04 | ms/batch 2618.20 | loss 2.69e-02\n",
            "| epoch   6 |   300/  417 batches | lr 2.00e-04 | ms/batch 1514.67 | loss 2.99e-02\n",
            "| epoch   6 |   310/  417 batches | lr 2.00e-04 | ms/batch 1706.59 | loss 2.37e-02\n",
            "| epoch   6 |   320/  417 batches | lr 2.00e-04 | ms/batch 1505.55 | loss 3.83e-03\n",
            "| epoch   6 |   330/  417 batches | lr 2.00e-04 | ms/batch 2624.47 | loss 2.65e-02\n",
            "| epoch   6 |   340/  417 batches | lr 2.00e-04 | ms/batch 1601.98 | loss 3.69e-02\n",
            "| epoch   6 |   350/  417 batches | lr 2.00e-04 | ms/batch 1579.86 | loss 2.88e-02\n",
            "| epoch   6 |   360/  417 batches | lr 2.00e-04 | ms/batch 1573.87 | loss 3.87e-03\n",
            "| epoch   6 |   370/  417 batches | lr 2.00e-04 | ms/batch 2893.26 | loss 6.00e-03\n",
            "| epoch   6 |   380/  417 batches | lr 2.00e-04 | ms/batch 1535.95 | loss 2.57e-03\n",
            "| epoch   6 |   390/  417 batches | lr 2.00e-04 | ms/batch 1508.83 | loss 2.00e-02\n",
            "| epoch   6 |   400/  417 batches | lr 2.00e-04 | ms/batch 1502.23 | loss 2.26e-02\n",
            "| epoch   6 |   410/  417 batches | lr 2.00e-04 | ms/batch 1919.60 | loss 2.80e-02\n",
            "|| min/epoch 13.01 | est. remaining time (h) 53.63 | loss 9.09e+00\n",
            "🚘 allpairs Epoch 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SO3ZpuXamzy2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}